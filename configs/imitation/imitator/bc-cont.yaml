defaults:
- _self_
- instance/demonstrations: default # Demonstrations loading type

# needs to convert all OmegaConf objects into dict/lists
_convert_: 'object' 

instance:
  _target_: intrl.algorithms.imitation.bc.bcensemble.BCEnsemble
  custom_logger: ${imitation.logger}
  policy_specs:
  - class: ${get_class:intrl.common.sb3.policies.ActorCriticPolicy}
    kwargs:
      observation_space: ${getattr:${env_spec.env.instance},observation_space}
      action_space: ${getattr:${env_spec.env.instance},action_space}
      lr_schedule:
        _target_: stable_baselines3.common.utils.constant_fn
        val: 0.001
      optimizer_kwargs:
        eps: 1e-08
      mlp_extractor_class: ${get_class:stable_baselines3.common.torch_layers.MlpExtractor}
      mlp_extractor_kwargs:
        feature_dim: Null # If left as null, ActorCriticPolicy will fill in automatically
        net_arch: 
          pi: [100, 100]
          vf: [100, 100]
        activation_fn: ${get_class:torch.nn.Tanh}
      dist_class: ${get_class:stable_baselines3.common.distributions.DiagGaussianDistribution}
      dist_kwargs:
        action_dim:
          _target_: stable_baselines3.common.preprocessing.get_action_dim
          action_space: ${rgetattr:${env_spec.env.instance},action_space}
  rng: 
    _target_: numpy.random.default_rng
    seed: Null # Same seed chosen if global seed is set
train_kwargs:
  n_epochs: 500