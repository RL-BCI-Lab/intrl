defaults:
- _self_
- instance/demonstrations: default # Demonstrations loading type

# needs to convert all OmegaConf objects into dict/lists
_convert_: 'object' 

instance:
  _target_: intrl.algorithms.imitation.bc.bcnoise.BCNoise
  split_demos: True
  copy_n_policies: Null
  copy_random_policies: True
  custom_logger: ${imitation.logger}
  policy_specs:
  - &policy # YAML anchor
    class: ${get_class:intrl.common.sb3.policies.ActorCriticPolicy}
    kwargs:
      observation_space: ${getattr:${env_spec.env.instance},observation_space}
      action_space: ${getattr:${env_spec.env.instance},action_space}
      lr_schedule:
        _target_: stable_baselines3.common.utils.constant_fn
        val: 0.001
      optimizer_kwargs:
        eps: 1e-08
      mlp_extractor_class: ${get_class:stable_baselines3.common.torch_layers.MlpExtractor}
      mlp_extractor_kwargs:
        feature_dim: Null # If left as null, ActorCriticPolicy will fill in automatically
        net_arch: 
          pi: [100, 100]
          vf: [100, 100]
        activation_fn: ${get_class:torch.nn.Tanh}
      dist_class: ${get_class:stable_baselines3.common.distributions.CategoricalDistribution}
      dist_kwargs:
        action_dim: ${rgetattr:${env_spec.env.instance},action_space.n}
  # Since policy_specs.0 does not build an single object at the top level, this works
  # and does not copy and paste a reference to the same object. Use anchors to be double sure.
  # - ${imitation.imitator.instance.policy_specs.0} 
  # - ${imitation.imitator.instance.policy_specs.0}
  # - ${imitation.imitator.instance.policy_specs.0}
  # - ${imitation.imitator.instance.policy_specs.0}
  - *policy # Dump YAMl anchor
  - *policy
  - *policy
  - *policy
  rng: 
    _target_: numpy.random.default_rng
    seed: Null # Same seed chosen if global seed is set
train_kwargs:
  # n_epochs: 500
  n_batches: 1000
  log_interval: 100